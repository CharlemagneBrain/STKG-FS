{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c5e7352",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-02 10:01:40.320705: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1770026503.279505 3516150 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1770026503.918795 3516150 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2026-02-02 10:01:48.408000: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/charles/stkgfs/stkgfs/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import ast\n",
    "import json\n",
    "from datasets import Dataset\n",
    "from transformers import CamembertTokenizerFast, DataCollatorForTokenClassification, CamembertForTokenClassification, TrainingArguments, Trainer\n",
    "import numpy as np\n",
    "from seqeval.metrics import classification_report\n",
    "import torch\n",
    "from torchsummary import summary\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6433984",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59900, 14758)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_json_safely(file_path):\n",
    "    try:\n",
    "        \n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            return json.load(f)\n",
    "    except:\n",
    "        try:\n",
    "            \n",
    "            data = []\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    try:\n",
    "                        \n",
    "                        data.append(json.loads(line.strip()))\n",
    "                    except:\n",
    "                        continue\n",
    "            return data\n",
    "        except:\n",
    "            \n",
    "            return pd.read_json(file_path, lines=True).to_dict('records')\n",
    "\n",
    "\n",
    "train_data = load_json_safely('./annotations/train_extended_bio_feb.json')\n",
    "test_data = load_json_safely('./annotations/val_extended_bio_feb.json')\n",
    "\n",
    "len(train_data), len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3005824b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 59900/59900 [00:07<00:00, 7493.14 examples/s]\n",
      "Map: 100%|██████████| 14758/14758 [00:01<00:00, 7604.18 examples/s]\n",
      "Map: 100%|██████████| 59900/59900 [01:09<00:00, 864.32 examples/s] \n",
      "Map: 100%|██████████| 14758/14758 [00:17<00:00, 827.48 examples/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = Dataset.from_dict({\"tokens\": [item[\"tokens\"] for item in train_data],\n",
    "                                   \"ner_tags\": [item[\"tags\"] for item in train_data]})\n",
    "test_dataset = Dataset.from_dict({\"tokens\": [item[\"tokens\"] for item in test_data],\n",
    "                                  \"ner_tags\": [item[\"tags\"] for item in test_data]})\n",
    "\n",
    "\n",
    "tokenizer = CamembertTokenizerFast.from_pretrained(\"camembert-base\")\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "\n",
    "label_list = [\"O\", \"B-country\", \"I-country\", \"B-region\", \"I-region\", \"B-departement\", \"I-departement\", \"B-province\", \"I-province\", \"B-village\", \"I-village\"]\n",
    "label2id = {label: i for i, label in enumerate(label_list)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "\n",
    "def convert_tags_to_ids(examples):\n",
    "    examples[\"ner_tags\"] = [[label2id[tag] for tag in tags] for tags in examples[\"ner_tags\"]]\n",
    "    return examples\n",
    "\n",
    "\n",
    "train_dataset = train_dataset.map(convert_tags_to_ids, batched=True)\n",
    "test_dataset = test_dataset.map(convert_tags_to_ids, batched=True)\n",
    "\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        is_split_into_words=True,\n",
    "        return_offsets_mapping=True\n",
    "    )\n",
    "\n",
    "    batch_labels  = []\n",
    "    batch_offsets = []\n",
    "\n",
    "\n",
    "    for i, labels in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids      = tokenized.word_ids(batch_index=i)\n",
    "        offsets_i     = tokenized[\"offset_mapping\"][i]\n",
    "        label_ids, offs = [], []\n",
    "\n",
    "        prev_word_idx = None\n",
    "        for idx, word_idx in enumerate(word_ids):\n",
    "            if word_idx is None:\n",
    "\n",
    "                label_ids.append(-100)\n",
    "                offs.append((None, None))\n",
    "            elif word_idx != prev_word_idx:\n",
    "\n",
    "                label_ids.append(labels[word_idx])\n",
    "                offs.append(offsets_i[idx])\n",
    "            else:\n",
    "\n",
    "                label_ids.append(-100)\n",
    "                offs.append((None, None))\n",
    "            prev_word_idx = word_idx\n",
    "\n",
    "        batch_labels.append(label_ids)\n",
    "        batch_offsets.append(offs)\n",
    "\n",
    "\n",
    "    tokenized[\"labels\"]       = batch_labels\n",
    "    tokenized[\"char_offsets\"] = batch_offsets\n",
    "\n",
    "\n",
    "\n",
    "    return tokenized\n",
    "\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_and_align_labels, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b0e608c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of CamembertForTokenClassification were not initialized from the model checkpoint at camembert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = CamembertForTokenClassification.from_pretrained(\"camembert-base\", num_labels=len(label_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0517f940",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CamembertForTokenClassification(\n",
       "  (roberta): CamembertModel(\n",
       "    (embeddings): CamembertEmbeddings(\n",
       "      (word_embeddings): Embedding(32005, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): CamembertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x CamembertLayer(\n",
       "          (attention): CamembertAttention(\n",
       "            (self): CamembertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): CamembertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): CamembertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): CamembertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=11, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3412f094",
   "metadata": {},
   "outputs": [],
   "source": "# Freeze embedding layers\nfor param in model.roberta.embeddings.parameters():\n    param.requires_grad = False"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfe3ca1",
   "metadata": {},
   "outputs": [],
   "source": "# Count trainable vs frozen parameters\ntotal_params = sum(p.numel() for p in model.parameters())\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nfrozen_params = total_params - trainable_params\n\nprint(f\"Total parameters: {total_params:,}\")\nprint(f\"Trainable parameters: {trainable_params:,}\")\nprint(f\"Frozen parameters: {frozen_params:,}\")\nprint(f\"Trainable percentage: {100 * trainable_params / total_params:.2f}%\")\n\n# Details per component\nprint(\"\\n--- Details per component ---\")\nfor name, module in model.named_children():\n    module_params = sum(p.numel() for p in module.parameters())\n    module_trainable = sum(p.numel() for p in module.parameters() if p.requires_grad)\n    print(f\"{name}: {module_trainable:,}/{module_params:,} trainable parameters\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62720ea1",
   "metadata": {},
   "outputs": [],
   "source": "# Training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"/data/charles/agile/camembert-ner-finetuned\",\n    evaluation_strategy=\"steps\",\n    learning_rate=5e-5,\n    per_device_train_batch_size=32,\n    per_device_eval_batch_size=32,\n    num_train_epochs=70,\n    weight_decay=0.01,\n    save_strategy=\"steps\",\n    save_steps = 100,\n    #\n    do_train=True,\n    do_predict=True,\n    save_total_limit=100,\n    push_to_hub=False,\n)\n\n\n\n# Compute evaluation metrics\ndef compute_metrics(p):\n    predictions, labels = p\n    predictions = np.argmax(predictions, axis=2)\n\n    true_labels = [[id2label[l] for l in label if l != -100] for label in labels]\n    true_predictions = [[id2label[p] for (p, l) in zip(prediction, label) if l != -100]\n                        for prediction, label in zip(predictions, labels)]\n\n    results = classification_report(true_labels, true_predictions, output_dict=True)\n    return {\n        \"precision\": results[\"micro avg\"][\"precision\"],\n        \"recall\": results[\"micro avg\"][\"recall\"],\n        \"f1\": results[\"micro avg\"][\"f1-score\"],\n    }\n\n# Initialize Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n)\n\ntrainer.train()"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ae1c6a77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.004938560537993908,\n",
       " 'eval_precision': 0.9345902626580812,\n",
       " 'eval_recall': 0.950933257918552,\n",
       " 'eval_f1': 0.9426909328785364}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15cfda6",
   "metadata": {},
   "outputs": [],
   "source": "import json\nfrom datasets import Dataset\nfrom transformers import CamembertTokenizerFast, DataCollatorForTokenClassification, CamembertForTokenClassification, Trainer\nimport numpy as np\nfrom seqeval.metrics import classification_report\n\n# Load JSON data\ndef load_json(file_path):\n    with open(file_path, 'r', encoding='utf-8') as f:\n        return json.load(f)\n\n# Load test data\ntest_data = load_json('./annotations/test_extended_bio_feb.json')\n\n# Convert to Dataset format\ntest_dataset = Dataset.from_dict({\n    \"tokens\": [item[\"tokens\"] for item in test_data],\n    \"ner_tags\": [item[\"tags\"] for item in test_data]\n})\n\n# Initialize tokenizer\ntokenizer = CamembertTokenizerFast.from_pretrained(\"camembert-base\")\n\n# Define label list and create label <-> id mappings\nlabel_list = [\"O\", \"B-country\", \"I-country\", \"B-region\", \"I-region\", \"B-departement\", \"I-departement\", \"B-province\", \"I-province\", \"B-village\", \"I-village\"]\nlabel2id = {label: i for i, label in enumerate(label_list)}\nid2label = {i: label for label, i in label2id.items()}\n\n# Convert string tags to numeric IDs\ndef convert_tags_to_ids(examples):\n    examples[\"ner_tags\"] = [[label2id[tag] for tag in tags] for tags in examples[\"ner_tags\"]]\n    return examples\n\n# Apply conversion on test dataset\ntest_dataset = test_dataset.map(convert_tags_to_ids, batched=True)\n\n# Tokenize inputs and align labels with subword tokens\ndef tokenize_and_align_labels(examples):\n    tokenized_inputs = tokenizer(\n        examples[\"tokens\"],\n        truncation=True,\n        padding=True,\n        is_split_into_words=True,\n    )\n\n    labels = []\n    for i, label in enumerate(examples[\"ner_tags\"]):\n        word_ids = tokenized_inputs.word_ids(batch_index=i)\n        previous_word_idx = None\n        label_ids = []\n        for word_idx in word_ids:\n            if word_idx is None:\n                label_ids.append(-100)  # Ignore special tokens\n            elif word_idx != previous_word_idx:\n                label_ids.append(label[word_idx])  # New word\n            else:\n                label_ids.append(-100)  # Same word, ignore\n            previous_word_idx = word_idx\n        labels.append(label_ids)\n\n    tokenized_inputs[\"labels\"] = labels\n    return tokenized_inputs\n\n# Apply tokenization and label alignment\ntest_dataset = test_dataset.map(tokenize_and_align_labels, batched=True)\n\n# Load fine-tuned model\nmodel = CamembertForTokenClassification.from_pretrained(\"/data/charles/agile/camembert-ner-finetuned/checkpoint-15000\", num_labels=len(label_list))\n\n# Initialize Trainer for evaluation only (no training)\ntrainer = Trainer(\n    model=model,\n    tokenizer=tokenizer,\n    data_collator=DataCollatorForTokenClassification(tokenizer),\n)\n\n# Run predictions on test set\npredictions, labels, _ = trainer.predict(test_dataset)\npredictions = np.argmax(predictions, axis=2)\n\n# Convert IDs back to labels\ntrue_labels = [[id2label[l] for l in label if l != -100] for label in labels]\ntrue_predictions = [[id2label[p] for (p, l) in zip(prediction, label) if l != -100]\n                    for prediction, label in zip(predictions, labels)]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab1e98c",
   "metadata": {},
   "outputs": [],
   "source": "# Classification report on test set\nprint(classification_report(true_labels, true_predictions))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c1abd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stkgfs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}