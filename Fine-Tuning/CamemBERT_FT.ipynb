{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c5e7352",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-02 10:01:40.320705: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1770026503.279505 3516150 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1770026503.918795 3516150 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2026-02-02 10:01:48.408000: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/charles/stkgfs/stkgfs/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import ast\n",
    "import json\n",
    "from datasets import Dataset\n",
    "from transformers import CamembertTokenizerFast, DataCollatorForTokenClassification, CamembertForTokenClassification, TrainingArguments, Trainer\n",
    "import numpy as np\n",
    "from seqeval.metrics import classification_report\n",
    "import torch\n",
    "from torchsummary import summary\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6433984",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59900, 14758)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_json_safely(file_path):\n",
    "    try:\n",
    "        \n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            return json.load(f)\n",
    "    except:\n",
    "        try:\n",
    "            \n",
    "            data = []\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    try:\n",
    "                        \n",
    "                        data.append(json.loads(line.strip()))\n",
    "                    except:\n",
    "                        continue\n",
    "            return data\n",
    "        except:\n",
    "            \n",
    "            return pd.read_json(file_path, lines=True).to_dict('records')\n",
    "\n",
    "\n",
    "train_data = load_json_safely('./annotations/train_extended_bio_feb.json')\n",
    "test_data = load_json_safely('./annotations/val_extended_bio_feb.json')\n",
    "\n",
    "len(train_data), len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3005824b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 59900/59900 [00:07<00:00, 7493.14 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 14758/14758 [00:01<00:00, 7604.18 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 59900/59900 [01:09<00:00, 864.32 examples/s] \n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 14758/14758 [00:17<00:00, 827.48 examples/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = Dataset.from_dict({\"tokens\": [item[\"tokens\"] for item in train_data],\n",
    "                                   \"ner_tags\": [item[\"tags\"] for item in train_data]})\n",
    "test_dataset = Dataset.from_dict({\"tokens\": [item[\"tokens\"] for item in test_data],\n",
    "                                  \"ner_tags\": [item[\"tags\"] for item in test_data]})\n",
    "\n",
    "\n",
    "tokenizer = CamembertTokenizerFast.from_pretrained(\"camembert-base\")\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "\n",
    "label_list = [\"O\", \"B-country\", \"I-country\", \"B-region\", \"I-region\", \"B-departement\", \"I-departement\", \"B-province\", \"I-province\", \"B-village\", \"I-village\"]\n",
    "label2id = {label: i for i, label in enumerate(label_list)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "\n",
    "def convert_tags_to_ids(examples):\n",
    "    examples[\"ner_tags\"] = [[label2id[tag] for tag in tags] for tags in examples[\"ner_tags\"]]\n",
    "    return examples\n",
    "\n",
    "\n",
    "train_dataset = train_dataset.map(convert_tags_to_ids, batched=True)\n",
    "test_dataset = test_dataset.map(convert_tags_to_ids, batched=True)\n",
    "\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        is_split_into_words=True,\n",
    "        return_offsets_mapping=True\n",
    "    )\n",
    "\n",
    "    batch_labels  = []\n",
    "    batch_offsets = []\n",
    "\n",
    "\n",
    "    for i, labels in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids      = tokenized.word_ids(batch_index=i)\n",
    "        offsets_i     = tokenized[\"offset_mapping\"][i]\n",
    "        label_ids, offs = [], []\n",
    "\n",
    "        prev_word_idx = None\n",
    "        for idx, word_idx in enumerate(word_ids):\n",
    "            if word_idx is None:\n",
    "\n",
    "                label_ids.append(-100)\n",
    "                offs.append((None, None))\n",
    "            elif word_idx != prev_word_idx:\n",
    "\n",
    "                label_ids.append(labels[word_idx])\n",
    "                offs.append(offsets_i[idx])\n",
    "            else:\n",
    "\n",
    "                label_ids.append(-100)\n",
    "                offs.append((None, None))\n",
    "            prev_word_idx = word_idx\n",
    "\n",
    "        batch_labels.append(label_ids)\n",
    "        batch_offsets.append(offs)\n",
    "\n",
    "\n",
    "    tokenized[\"labels\"]       = batch_labels\n",
    "    tokenized[\"char_offsets\"] = batch_offsets\n",
    "\n",
    "\n",
    "\n",
    "    return tokenized\n",
    "\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_and_align_labels, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b0e608c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of CamembertForTokenClassification were not initialized from the model checkpoint at camembert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = CamembertForTokenClassification.from_pretrained(\"camembert-base\", num_labels=len(label_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0517f940",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CamembertForTokenClassification(\n",
       "  (roberta): CamembertModel(\n",
       "    (embeddings): CamembertEmbeddings(\n",
       "      (word_embeddings): Embedding(32005, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): CamembertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x CamembertLayer(\n",
       "          (attention): CamembertAttention(\n",
       "            (self): CamembertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): CamembertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): CamembertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): CamembertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=11, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3412f094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geler les embeddings\n",
    "for param in model.roberta.embeddings.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# #Geler les premi√®res couches du CamembertEncoder (par exemple, les 8 premi√®res sur 12)\n",
    "# for layer in model.roberta.encoder.layer[:-4]:  # Ne pas entra√Æner les 8 premi√®res couches\n",
    "#     for param in layer.parameters():\n",
    "#         param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2bfe3ca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de param√®tres: 110,039,819\n",
      "Param√®tres entra√Ænables: 85,062,923\n",
      "Param√®tres gel√©s: 24,976,896\n",
      "Pourcentage entra√Ænable: 77.30%\n",
      "\n",
      "--- D√©tails par composant ---\n",
      "roberta: 85,054,464/110,031,360 param√®tres entra√Ænables\n",
      "dropout: 0/0 param√®tres entra√Ænables\n",
      "classifier: 8,459/8,459 param√®tres entra√Ænables\n"
     ]
    }
   ],
   "source": [
    "# Compter les param√®tres entra√Ænables vs gel√©s\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "frozen_params = total_params - trainable_params\n",
    "\n",
    "print(f\"Total de param√®tres: {total_params:,}\")\n",
    "print(f\"Param√®tres entra√Ænables: {trainable_params:,}\")\n",
    "print(f\"Param√®tres gel√©s: {frozen_params:,}\")\n",
    "print(f\"Pourcentage entra√Ænable: {100 * trainable_params / total_params:.2f}%\")\n",
    "\n",
    "# D√©tails par composant\n",
    "print(\"\\n--- D√©tails par composant ---\")\n",
    "for name, module in model.named_children():\n",
    "    module_params = sum(p.numel() for p in module.parameters())\n",
    "    module_trainable = sum(p.numel() for p in module.parameters() if p.requires_grad)\n",
    "    print(f\"{name}: {module_trainable:,}/{module_params:,} param√®tres entra√Ænables\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62720ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/charles/stkgfs/stkgfs/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5091' max='131040' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  5091/131040 2:10:01 < 53:38:05, 0.65 it/s, Epoch 2.72/70]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.246000</td>\n",
       "      <td>0.032766</td>\n",
       "      <td>0.506716</td>\n",
       "      <td>0.537000</td>\n",
       "      <td>0.521419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.023800</td>\n",
       "      <td>0.016327</td>\n",
       "      <td>0.625757</td>\n",
       "      <td>0.706307</td>\n",
       "      <td>0.663596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.012700</td>\n",
       "      <td>0.007961</td>\n",
       "      <td>0.897804</td>\n",
       "      <td>0.940375</td>\n",
       "      <td>0.918597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.006300</td>\n",
       "      <td>0.005177</td>\n",
       "      <td>0.918545</td>\n",
       "      <td>0.950886</td>\n",
       "      <td>0.934436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.004100</td>\n",
       "      <td>0.003550</td>\n",
       "      <td>0.941017</td>\n",
       "      <td>0.962528</td>\n",
       "      <td>0.951651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>0.002708</td>\n",
       "      <td>0.959306</td>\n",
       "      <td>0.961114</td>\n",
       "      <td>0.960209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.002400</td>\n",
       "      <td>0.002389</td>\n",
       "      <td>0.943823</td>\n",
       "      <td>0.970871</td>\n",
       "      <td>0.957156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>0.951889</td>\n",
       "      <td>0.976386</td>\n",
       "      <td>0.963982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.001770</td>\n",
       "      <td>0.963536</td>\n",
       "      <td>0.975207</td>\n",
       "      <td>0.969336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.001739</td>\n",
       "      <td>0.969349</td>\n",
       "      <td>0.971908</td>\n",
       "      <td>0.970627</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/charles/stkgfs/stkgfs/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/charles/stkgfs/stkgfs/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# D√©finir les arguments d'entra√Ænement\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/data/charles/agile/camembert-ner-finetuned\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=70,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps = 100,\n",
    "    #\n",
    "    do_train=True,\n",
    "    do_predict=True,\n",
    "    save_total_limit=100,\n",
    "    push_to_hub=False,\n",
    "    #metric_for_best_model=\"eval_loss\",\n",
    "    #load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Fonction pour calculer les m√©triques\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_labels = [[id2label[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [[id2label[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "                        for prediction, label in zip(predictions, labels)]\n",
    "\n",
    "    results = classification_report(true_labels, true_predictions, output_dict=True)\n",
    "    return {\n",
    "        \"precision\": results[\"micro avg\"][\"precision\"],\n",
    "        \"recall\": results[\"micro avg\"][\"recall\"],\n",
    "        \"f1\": results[\"micro avg\"][\"f1-score\"],\n",
    "    }\n",
    "\n",
    "# Initialiser le Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ae1c6a77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.004938560537993908,\n",
       " 'eval_precision': 0.9345902626580812,\n",
       " 'eval_recall': 0.950933257918552,\n",
       " 'eval_f1': 0.9426909328785364}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b15cfda6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11594/11594 [00:01<00:00, 7610.18 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11594/11594 [00:06<00:00, 1909.60 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "from datasets import Dataset\n",
    "from transformers import CamembertTokenizerFast, DataCollatorForTokenClassification, CamembertForTokenClassification, Trainer\n",
    "import numpy as np\n",
    "from seqeval.metrics import classification_report\n",
    "\n",
    "# Fonction pour charger les donn√©es JSON\n",
    "def load_json(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# Charger les donn√©es de test\n",
    "test_data = load_json('./annotations/test_extended_bio_feb.json')\n",
    "\n",
    "# Convertir en format Dataset\n",
    "test_dataset = Dataset.from_dict({\n",
    "    \"tokens\": [item[\"tokens\"] for item in test_data],\n",
    "    \"ner_tags\": [item[\"tags\"] for item in test_data]\n",
    "})\n",
    "\n",
    "# Initialiser le tokenizer\n",
    "tokenizer = CamembertTokenizerFast.from_pretrained(\"camembert-base\")\n",
    "\n",
    "# D√©finir la liste des √©tiquettes et cr√©er les mappings label <-> id\n",
    "label_list = [\"O\", \"B-country\", \"I-country\", \"B-region\", \"I-region\", \"B-departement\", \"I-departement\", \"B-province\", \"I-province\", \"B-village\", \"I-village\"]\n",
    "label2id = {label: i for i, label in enumerate(label_list)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "\n",
    "# Fonction pour convertir les √©tiquettes en IDs\n",
    "def convert_tags_to_ids(examples):\n",
    "    examples[\"ner_tags\"] = [[label2id[tag] for tag in tags] for tags in examples[\"ner_tags\"]]\n",
    "    return examples\n",
    "\n",
    "# Appliquer la conversion sur le dataset de test\n",
    "test_dataset = test_dataset.map(convert_tags_to_ids, batched=True)\n",
    "\n",
    "# Fonction pour tokeniser les entr√©es et aligner les √©tiquettes\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        is_split_into_words=True,\n",
    "    )\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)  # Ignorer les tokens sp√©ciaux\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])  # Nouveau mot\n",
    "            else:\n",
    "                label_ids.append(-100)  # M√™me mot, ignorer\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "# Appliquer la tokenisation et l'alignement des √©tiquettes\n",
    "test_dataset = test_dataset.map(tokenize_and_align_labels, batched=True)\n",
    "\n",
    "# Charger le mod√®le fine-tun√©\n",
    "model = CamembertForTokenClassification.from_pretrained(\"/data/charles/agile/camembert-ner-finetuned/checkpoint-15000\", num_labels=len(label_list))\n",
    "\n",
    "# Initialiser le Trainer sans entra√Ænement, uniquement pour l'√©valuation\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorForTokenClassification(tokenizer),\n",
    ")\n",
    "\n",
    "# Effectuer les pr√©dictions sur le jeu de test\n",
    "predictions, labels, _ = trainer.predict(test_dataset)\n",
    "predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "# Convertir les IDs en √©tiquettes\n",
    "true_labels = [[id2label[l] for l in label if l != -100] for label in labels]\n",
    "true_predictions = [[id2label[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "                    for prediction, label in zip(predictions, labels)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ab1e98c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     country       0.99      0.99      0.99      4648\n",
      " departement       0.99      0.99      0.99      6744\n",
      "    province       0.99      0.98      0.99       541\n",
      "      region       1.00      0.99      0.99      1433\n",
      "     village       0.94      0.93      0.94      3236\n",
      "\n",
      "   micro avg       0.98      0.98      0.98     16602\n",
      "   macro avg       0.98      0.98      0.98     16602\n",
      "weighted avg       0.98      0.98      0.98     16602\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Afficher le rapport de classification\n",
    "print(classification_report(true_labels, true_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c1abd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stkgfs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
