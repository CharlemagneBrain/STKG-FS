{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3d84a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6dc2cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Optional, Tuple, Set\n",
    "\n",
    "# Liste des termes vagues à traiter spécialement\n",
    "VAGUE_TERMS = ['production', 'campagne', 'prix', 'coût', 'crise', 'stock', 'pluie', 'pluiviométrie']\n",
    "\n",
    "# Définition des triggers d'augmentation et de diminution\n",
    "# 'plus', 'meilleur', 'meilleure', 'supérieur', 'supérieure'\n",
    "# 'renforcement', 'renforce', 'renforcent', 'renforcer'\n",
    "AUGMENTATION_TRIGGERS = [\n",
    "    'augmentation', 'augmenté', 'augmente', 'augmentent', 'augmenter', 'augmentée', 'augmentées',\n",
    "    'hausse', 'hausse', 'haussier', 'haussière', 'hausse', 'haussent',\n",
    "    'accroissement', 'accroître', 'accroît', 'accru', 'accrue',\n",
    "    'progression', 'progresse', 'progressent', 'progresser',\n",
    "    'croissance', 'croît', 'croissant', 'croissante',\n",
    "    'amélioration', 'améliore', 'améliorent', 'améliorer',\n",
    "    'montée', 'monte', 'montent', 'monter',\n",
    "    'élévation', 'élève', 'élèvent', 'élever',\n",
    "    'intensification', 'intensifie', 'intensifient', 'intensifier'\n",
    "]\n",
    "DIMINUTION_TRIGGERS = [\n",
    "    'diminution', 'diminué', 'diminue', 'diminuent', 'diminuer',\n",
    "    'baisse', 'baissé', 'baisse', 'baissent', 'baisser',\n",
    "    'réduction', 'réduit', 'réduite', 'réduisent', 'réduire',\n",
    "    'chute', 'chuté', 'chute', 'chutent', 'chuter',\n",
    "    'déclin', 'décline', 'déclinent', 'décliner',\n",
    "    'recul', 'recule', 'reculent', 'reculer',\n",
    "    'régression', 'régresse', 'régressent', 'régresser',\n",
    "    'dégradation', 'dégrade', 'dégradent', 'dégrader',\n",
    "    'détérioration', 'détériore', 'détériorent', 'détériorer',\n",
    "    'affaiblissement', 'affaibli', 'affaiblie', 'affaiblissent', 'affaiblir',\n",
    "    'moins', 'pire', 'inférieur', 'inférieure', 'moindre'\n",
    "]\n",
    "\n",
    "ALL_TRIGGERS = set(AUGMENTATION_TRIGGERS + DIMINUTION_TRIGGERS)\n",
    "def generate_term_variations(term: str) -> List[str]:\n",
    "    variations = [\n",
    "        term,  \n",
    "        term + 's',  \n",
    "        term + 'es',  \n",
    "        term + \"'\",  \n",
    "    ]\n",
    "    return variations\n",
    "\n",
    "def generate_trigger_variations() -> Set[str]:\n",
    "    all_variations = set()\n",
    "    for trigger in ALL_TRIGGERS:\n",
    "        variations = generate_term_variations(trigger)\n",
    "        for var in variations:\n",
    "            all_variations.add(var.lower())\n",
    "    return all_variations\n",
    "\n",
    "# Initialisation des variations de triggers\n",
    "TRIGGER_VARIATIONS = generate_trigger_variations()\n",
    "\n",
    "class PhraseContext:\n",
    "    def __init__(self, text: str, terms: List[Dict], locations: List[Dict], dates: List[Dict]):\n",
    "        self.text = text\n",
    "        self.terms = terms\n",
    "        self.locations = locations\n",
    "        self.dates = dates\n",
    "        \n",
    "    @property\n",
    "    def has_terms(self) -> bool:\n",
    "        return len(self.terms) > 0\n",
    "    \n",
    "    @property\n",
    "    def has_entities(self) -> bool:\n",
    "        return len(self.locations) > 0 or len(self.dates) > 0\n",
    "    \n",
    "    def has_non_vague_terms(self) -> bool:\n",
    "        \"\"\"Vérifie s'il existe des termes non vagues dans le contexte\"\"\"\n",
    "        return any(not term['is_vague'] for term in self.terms)\n",
    "    \n",
    "    def has_trigger_terms(self) -> bool:\n",
    "        \"\"\"Vérifie s'il existe des termes de triggers dans le contexte\"\"\"\n",
    "        # Vérification de la présence de mots triggers dans le texte\n",
    "        words = re.findall(r'\\b\\w+\\b', self.text.lower())\n",
    "        return any(word in TRIGGER_VARIATIONS for word in words)\n",
    "\n",
    "def get_location_hierarchy_rank(label: str) -> int:\n",
    "    \"\"\"Définit la hiérarchie des lieux du plus petit au plus grand.\"\"\"\n",
    "    hierarchy = {\n",
    "        'village': 1,\n",
    "        'departement': 2,\n",
    "        'province': 3,\n",
    "        'region': 4,\n",
    "        'country': 5\n",
    "    }\n",
    "    return hierarchy.get(label, 0)\n",
    "\n",
    "def find_closest_date(dates: List[Dict], publication_date: str) -> Optional[Dict]:\n",
    "    if not dates:\n",
    "        return None\n",
    "    try:\n",
    "        pub_date_dt = datetime.strptime(publication_date, \"%Y-%m-%d\")\n",
    "    except ValueError:\n",
    "        return dates[0]\n",
    "    \n",
    "    valid_dates = []\n",
    "    for d in dates:\n",
    "        try:\n",
    "            dt = datetime.strptime(d['date'], \"%Y-%m-%d\")\n",
    "            if dt <= pub_date_dt:\n",
    "                valid_dates.append(d)\n",
    "        except ValueError:\n",
    "            pass\n",
    "    \n",
    "    if not valid_dates:\n",
    "        return None\n",
    "    \n",
    "    closest_dt = min(valid_dates, key=lambda x: abs(datetime.strptime(x['date'], \"%Y-%m-%d\") - pub_date_dt))\n",
    "    return closest_dt\n",
    "\n",
    "def find_terms(phrase: str, lexique_df: pd.DataFrame) -> List[Dict]:\n",
    "    term_variations = {}\n",
    "    for term in lexique_df['term'].str.lower().unique():\n",
    "        # Pour chaque terme, générer ses variations\n",
    "        variations = generate_term_variations(term)\n",
    "        for variation in variations:\n",
    "            term_variations[variation] = term\n",
    "    \n",
    "    # Créer le pattern regex\n",
    "    pattern = r'\\b(' + '|'.join(map(re.escape, term_variations.keys())) + r')\\b'\n",
    "    matches = re.finditer(pattern, phrase.lower())\n",
    "    results_terms = []\n",
    "    \n",
    "    for m in matches:\n",
    "        base_variation = m.group(1).lower()\n",
    "        # Retrouver le terme original\n",
    "        base_term = term_variations[base_variation]\n",
    "        \n",
    "        # Rechercher dans le lexique\n",
    "        original_rows = lexique_df[lexique_df['term'].str.lower() == base_term]\n",
    "        \n",
    "        if original_rows.empty:\n",
    "            continue\n",
    "        \n",
    "        original_row = original_rows.iloc[0].to_dict()\n",
    "        \n",
    "        # Préparation de l'entrée de terme\n",
    "        term_entry = {\n",
    "            'term': original_row['term'],  # Le terme original du lexique\n",
    "            'concept': original_row['concept'],\n",
    "            'theme': original_row['theme'],\n",
    "            'phase': original_row['phase'],  # Ajout de la phase\n",
    "            'position': (m.start(), m.end()),\n",
    "            'is_vague': base_term.lower() in VAGUE_TERMS\n",
    "        }\n",
    "        \n",
    "        results_terms.append(term_entry)\n",
    "    \n",
    "    return results_terms\n",
    "\n",
    "def create_result_entry(article_id: str, term_info: Dict, location: Optional[Dict], \n",
    "                       date: Optional[Dict], phrase: str, contexte_enrichi: str,\n",
    "                       companion_term_info: Optional[Dict] = None) -> Dict:\n",
    "    \n",
    "    # Gestion des termes et de leurs concepts\n",
    "    if companion_term_info:\n",
    "        # Si les concepts sont différents, on les concatène\n",
    "        if term_info['concept'] != companion_term_info['concept']:\n",
    "            combined_concept = f\"{term_info['concept']} + {companion_term_info['concept']}\"\n",
    "        else:\n",
    "            combined_concept = term_info['concept']\n",
    "        \n",
    "        # Même chose pour les thèmes\n",
    "        if term_info['theme'] != companion_term_info['theme']:\n",
    "            combined_theme = f\"{term_info['theme']} + {companion_term_info['theme']}\"\n",
    "        else:\n",
    "            combined_theme = term_info['theme']\n",
    "        \n",
    "        # Concaténation des termes\n",
    "        combined_term = f\"{term_info['term']}, {companion_term_info['term']}\"\n",
    "        \n",
    "        # Gestion de la phase - prendre la phase du terme principal\n",
    "        phase = term_info.get('phase', None)\n",
    "    else:\n",
    "        combined_concept = term_info['concept']\n",
    "        combined_theme = term_info['theme']\n",
    "        combined_term = term_info['term']\n",
    "        phase = term_info.get('phase', None)\n",
    "        \n",
    "    result_entry = {\n",
    "        'article_id': article_id,\n",
    "        'term': combined_term,\n",
    "        'concept': combined_concept,\n",
    "        'theme': combined_theme,\n",
    "        'phase': phase,  \n",
    "        'location': location,\n",
    "        'date': date['date'] if date else None,\n",
    "        'date_span': date['span'] if date else None,\n",
    "        'phrase': phrase,  # Phrase originale\n",
    "        'contexte_enrichi': contexte_enrichi,  # Nouveau champ pour le contexte enrichi\n",
    "        'complete_triplet': bool(location and date)\n",
    "    }\n",
    "    return result_entry\n",
    "\n",
    "def count_sentences(text: str) -> int:\n",
    "    \"\"\"Compte approximativement le nombre de phrases dans un texte.\"\"\"\n",
    "    # Cette fonction est simple mais pourrait être améliorée selon la complexité réelle de vos phrases\n",
    "    sentences = re.split(r'[.!?]+', text.strip())\n",
    "    # Filtrer les chaînes vides\n",
    "    sentences = [s for s in sentences if s.strip()]\n",
    "    return len(sentences)\n",
    "\n",
    "# def get_context_sentences(all_phrases: List[str], current_index: int, phrase: str) -> str:\n",
    "#     \"\"\"\n",
    "#     Crée un contexte enrichi en ajoutant des phrases avant et après la phrase courante.\n",
    "#     Si la phrase contient une seule phrase: ajoute 2 avant et 2 après\n",
    "#     Si la phrase contient plusieurs phrases: ajoute 1 avant et 1 après\n",
    "#     \"\"\"\n",
    "#     # Estimer si la phrase courante contient une ou plusieurs phrases\n",
    "#     sentence_count = count_sentences(phrase)\n",
    "    \n",
    "#     if sentence_count <= 1:\n",
    "#         # Pour une seule phrase, ajouter 2 avant et 2 après\n",
    "#         before_count = 2\n",
    "#         after_count = 2\n",
    "#     else:\n",
    "#         # Pour plusieurs phrases, ajouter 1 avant et 1 après\n",
    "#         before_count = 1\n",
    "#         after_count = 1\n",
    "    \n",
    "#     # Calculer les indices de début et de fin\n",
    "#     start_idx = max(0, current_index - before_count)\n",
    "#     end_idx = min(len(all_phrases), current_index + after_count + 1)\n",
    "    \n",
    "#     # Construire le contexte enrichi\n",
    "#     context_sentences = all_phrases[start_idx:end_idx]\n",
    "#     enriched_context = \" \".join(context_sentences)\n",
    "    \n",
    "#     return enriched_context\n",
    "\n",
    "def get_context_sentences(all_phrases: List[str], current_index: int, phrase: str) -> str:\n",
    "    \"\"\"\n",
    "    Crée un contexte enrichi en ajoutant une phrase avant et une phrase après la phrase courante.\n",
    "    \"\"\"\n",
    "    # Toujours ajouter 1 phrase avant et 1 phrase après, peu importe le contenu de la phrase courante\n",
    "    before_count = 1\n",
    "    after_count = 1\n",
    "    \n",
    "    # Calculer les indices de début et de fin\n",
    "    start_idx = max(0, current_index - before_count)\n",
    "    end_idx = min(len(all_phrases), current_index + after_count + 1)  # +1 car on inclut l'index courant\n",
    "    \n",
    "    # Construire le contexte enrichi\n",
    "    context_sentences = all_phrases[start_idx:end_idx]\n",
    "    enriched_context = \" \".join(context_sentences)\n",
    "    \n",
    "    return enriched_context\n",
    "\n",
    "def process_reconstructed_articles(reconstruct_df: pd.DataFrame, lexique_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    results = []\n",
    "    # Pour chaque article reconstruit\n",
    "    for idx, row in reconstruct_df.iterrows():\n",
    "        article_id = row['article_id']\n",
    "        full_text = row['texte_complet']\n",
    "        publication_date = row.get('publication_date', row.get('annee'))\n",
    "        \n",
    "        # Segmentation en phrases avec SaT\n",
    "        all_phrases = row['sentences']\n",
    "        all_phrases = [phrase.strip() for phrase in all_phrases if phrase.strip()]\n",
    "        \n",
    "        # Construction des contextes de phrases\n",
    "        phrase_contexts = []\n",
    "        pos = 0\n",
    "        for phrase in all_phrases:\n",
    "            if not phrase:\n",
    "                continue\n",
    "            \n",
    "            phrase_start = full_text.find(phrase, pos)\n",
    "            if phrase_start == -1:\n",
    "                continue\n",
    "                \n",
    "            phrase_end = phrase_start + len(phrase)\n",
    "            pos = phrase_end\n",
    "            \n",
    "            # Extraction des annotations spatiales et temporelles\n",
    "            current_locations = []\n",
    "            if isinstance(row['gold_annotations'], list):\n",
    "                for loc in row['gold_annotations']:\n",
    "                    if loc['start'] >= phrase_start and loc['end'] <= phrase_end:\n",
    "                        if not any(existing_loc['text'] == loc['text'] for existing_loc in current_locations):\n",
    "                            current_locations.append(loc)\n",
    "            \n",
    "            current_dates = []\n",
    "            if isinstance(row['heideltime_dates_with_spans'], list):\n",
    "                for date_ann in row['heideltime_dates_with_spans']:\n",
    "                    local_start, local_end = date_ann.get('span', (0, 0))\n",
    "                    if local_start >= phrase_start and local_end <= phrase_end:\n",
    "                        if not any(d['date'] == date_ann['date'] for d in current_dates):\n",
    "                            current_dates.append({\n",
    "                                'date': date_ann['date'],\n",
    "                                'span': date_ann['span']\n",
    "                            })\n",
    "            \n",
    "            # Recherche des termes dans la phrase\n",
    "            current_terms = find_terms(phrase, lexique_df)\n",
    "            \n",
    "            phrase_contexts.append(PhraseContext(\n",
    "                text=phrase,\n",
    "                terms=current_terms,\n",
    "                locations=current_locations,\n",
    "                dates=current_dates\n",
    "            ))\n",
    "        \n",
    "        # Projections spatiales et temporelles\n",
    "        num_phrases = len(phrase_contexts)\n",
    "        spatial_proj = [None] * num_phrases\n",
    "        \n",
    "        for i, ctx in enumerate(phrase_contexts):\n",
    "            if ctx.locations:\n",
    "                spatial_proj[i] = min(ctx.locations, key=lambda x: get_location_hierarchy_rank(x['label']))\n",
    "        \n",
    "        for i in range(num_phrases):\n",
    "            if spatial_proj[i] is not None:\n",
    "                for j in [i+1, i+2]:\n",
    "                    if j < num_phrases and spatial_proj[j] is None:\n",
    "                        spatial_proj[j] = spatial_proj[i]\n",
    "        \n",
    "        default_spatial = {'label': 'country', 'text': 'Burkina'}\n",
    "        for i in range(num_phrases):\n",
    "            if spatial_proj[i] is None:\n",
    "                spatial_proj[i] = default_spatial\n",
    "        \n",
    "        temporal_proj = [None] * num_phrases\n",
    "        last_temp = None\n",
    "        \n",
    "        for i, ctx in enumerate(phrase_contexts):\n",
    "            if ctx.dates:\n",
    "                best_date = find_closest_date(ctx.dates, publication_date)\n",
    "                temporal_proj[i] = best_date\n",
    "                last_temp = best_date\n",
    "            else:\n",
    "                temporal_proj[i] = last_temp\n",
    "        \n",
    "        for i in range(num_phrases):\n",
    "            if temporal_proj[i] is None:\n",
    "                temporal_proj[i] = {'date': publication_date, 'span': (0, 0)}\n",
    "        \n",
    "        # Nouvelle logique de traitement des termes\n",
    "        for i, current_context in enumerate(phrase_contexts):\n",
    "            if current_context.has_terms:\n",
    "                current_best_location = spatial_proj[i]\n",
    "                current_best_date = temporal_proj[i]\n",
    "                original_phrase = current_context.text\n",
    "                \n",
    "                # Enrichir la phrase avec les phrases contextuelles\n",
    "                contexte_enrichi = get_context_sentences(all_phrases, i, original_phrase)\n",
    "                \n",
    "                for j, term_info in enumerate(current_context.terms):\n",
    "                    # Traitement spécial pour les termes vagues\n",
    "                    if term_info['is_vague']:\n",
    "                        # Chercher un terme non vague dans le contexte local\n",
    "                        companion_term = next((t for t in current_context.terms \n",
    "                                               if not t['is_vague'] and t['term'] != term_info['term']), None)\n",
    "                        \n",
    "                        # Si pas de terme accompagnant dans le contexte local, on vérifie les triggers\n",
    "                        if not companion_term:\n",
    "                            # Vérifier si un trigger d'augmentation ou de diminution est présent\n",
    "                            if not current_context.has_trigger_terms():\n",
    "                                # Pas de trigger, on ignore ce terme vague\n",
    "                                continue\n",
    "                            # Sinon, on garde le terme vague sans terme accompagnant\n",
    "                            result_entry = create_result_entry(\n",
    "                                article_id=article_id,\n",
    "                                term_info=term_info,\n",
    "                                location=current_best_location,\n",
    "                                date=current_best_date,\n",
    "                                phrase=original_phrase,  # Phrase originale\n",
    "                                contexte_enrichi=contexte_enrichi  # Contexte enrichi dans une colonne séparée\n",
    "                            )\n",
    "                        else:\n",
    "                            # Cas normal avec terme accompagnant\n",
    "                            result_entry = create_result_entry(\n",
    "                                article_id=article_id,\n",
    "                                term_info=term_info,\n",
    "                                location=current_best_location,\n",
    "                                date=current_best_date,\n",
    "                                phrase=original_phrase,  # Phrase originale\n",
    "                                contexte_enrichi=contexte_enrichi,  # Contexte enrichi dans une colonne séparée\n",
    "                                companion_term_info=companion_term\n",
    "                            )\n",
    "                    else:\n",
    "                        # Pour les termes non vagues, comportement standard\n",
    "                        result_entry = create_result_entry(\n",
    "                            article_id=article_id,\n",
    "                            term_info=term_info,\n",
    "                            location=current_best_location,\n",
    "                            date=current_best_date,\n",
    "                            phrase=original_phrase,  # Phrase originale\n",
    "                            contexte_enrichi=contexte_enrichi  # Contexte enrichi dans une colonne séparée\n",
    "                        )\n",
    "                    \n",
    "                    # Vérification des doublons\n",
    "                    is_duplicate = any(\n",
    "                        existing_result['article_id'] == result_entry['article_id'] and\n",
    "                        existing_result['term'] == result_entry['term'] and\n",
    "                        existing_result['location'] == result_entry['location'] and\n",
    "                        existing_result['date'] == result_entry['date'] and \n",
    "                        existing_result['date_span'] == result_entry['date_span']\n",
    "                        for existing_result in results\n",
    "                    )\n",
    "                    \n",
    "                    if not is_duplicate:\n",
    "                        results.append(result_entry)\n",
    "    \n",
    "    return pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15dda38",
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstruct_df_new = pd.read_csv(\"/Users/charlesabdoulayengom/Documents/GeoTextAI_Pipeline/NER_GeoTEXTAI/results_csv/reconstruct_df_new.csv\")\n",
    "reconstruct_df_new"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
